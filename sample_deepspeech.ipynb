{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unsupervise abstract generate.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FisherCh/pytorch-tutorial/blob/master/sample_deepspeech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "3S5h5-AtlXrj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "# --------------------- \n",
        "# 作者：LeoWood \n",
        "# 来源：CSDN \n",
        "# 原文：https://blog.csdn.net/u014108004/article/details/84325301 \n",
        "# 版权声明：本文为博主原创文章，转载请附上博文链接！"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7uopxMIDlgQU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 挂载谷歌云端硬盘\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive  -o nonempty"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D0p_W1NjuyBo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# requirements\n",
        " - pytorch v1.0\n",
        " - warp-ctc\n",
        " - pytorch-audio\n",
        " - deepspeech"
      ]
    },
    {
      "metadata": {
        "id": "70tlpdPVvljv",
        "colab_type": "code",
        "outputId": "1cbd0729-36c7-4323-883e-6e9190c14b15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install python_Levenshtein"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python_Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\r\u001b[K    21% |██████▊                         | 10kB 19.1MB/s eta 0:00:01\r\u001b[K    42% |█████████████▌                  | 20kB 1.8MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▏           | 30kB 2.7MB/s eta 0:00:01\r\u001b[K    84% |███████████████████████████     | 40kB 1.8MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 51kB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python_Levenshtein) (40.6.3)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Running setup.py bdist_wheel for python-Levenshtein ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X3T59667OiRq",
        "colab_type": "code",
        "outputId": "7d677501-f122-4120-b021-0686af7f47e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install torch torchvision\n",
        "\n",
        "# !conda install pytorch torchvision -c pytorch\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 31kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x612e8000 @  0x7f81094d42a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 25.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/5e/e91792f198bbc5a0d7d3055ad552bc4062942d27eaf75c3e2783cf64eae5/Pillow-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Installing collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.4.1 torch-1.0.0 torchvision-0.2.1\n",
            "1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n8U9BHzPgRtD",
        "colab_type": "code",
        "outputId": "5e4dae65-d76b-4639-b5a9-28f0a1d29f88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "cell_type": "code",
      "source": [
        "# !apt-get install sox libsox-dev libsox-fmt-all\n",
        "# !git clone https://github.com/pytorch/audio.git\n",
        "# !pip install cffi\n",
        "# !python audio/setup.py install\n",
        "!pip install git+git://github.com/pytorch/audio\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/pytorch/audio\n",
            "  Cloning git://github.com/pytorch/audio to /tmp/pip-req-build-w83719pz\n",
            "Building wheels for collected packages: torchaudio\n",
            "  Running setup.py bdist_wheel for torchaudio ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-ot2su_4y/wheels/4e/7c/c5/0d946acbaccad9fe62590374454c4cf135846c9c96fce3ac75\n",
            "Successfully built torchaudio\n",
            "Installing collected packages: torchaudio\n",
            "Successfully installed torchaudio-0.2\n",
            "audio  build  sample_data  torchaudio.egg-info\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A3homg41s0uQ",
        "colab_type": "code",
        "outputId": "af10630f-a796-40e7-fb6e-8b9c931146b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "cell_type": "code",
      "source": [
        "# example of audio\n",
        "import torchaudio\n",
        "sound, sample_rate = torchaudio.load('foo.mp3')\n",
        "torchaudio.save('foo_save.mp3', sound, sample_rate)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-32632604a8ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'foo.mp3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'foo_save.mp3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchaudio/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filepath, out, normalization, channels_first, num_frames, offset, signalinfo, encodinginfo, filetype)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# check if valid file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not found or is a directory\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# initialize output tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: foo.mp3 not found or is a directory"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "nuAFLpCqhEMJ",
        "colab_type": "code",
        "outputId": "5cab5afe-68b8-49a4-afd5-207b5f7c54a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        }
      },
      "cell_type": "code",
      "source": [
        "# Install baidu wrap-ctc\n",
        "# !git clone https://github.com/SeanNaren/warp-ctc.git\n",
        "# !pwd\n",
        "# !ls\n",
        "# !cd warp-ctc;mkdir build; cd build; pwd; cmake ..; make\n",
        "!cd warp-ctc; cd pytorch_binding; python setup.py install"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating warpctc_pytorch.egg-info\n",
            "writing warpctc_pytorch.egg-info/PKG-INFO\n",
            "writing dependency_links to warpctc_pytorch.egg-info/dependency_links.txt\n",
            "writing top-level names to warpctc_pytorch.egg-info/top_level.txt\n",
            "writing manifest file 'warpctc_pytorch.egg-info/SOURCES.txt'\n",
            "reading manifest file 'warpctc_pytorch.egg-info/SOURCES.txt'\n",
            "writing manifest file 'warpctc_pytorch.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "creating build/lib.linux-x86_64-3.6/warpctc_pytorch\n",
            "copying warpctc_pytorch/__init__.py -> build/lib.linux-x86_64-3.6/warpctc_pytorch\n",
            "running build_ext\n",
            "building 'warpctc_pytorch._warp_ctc' extension\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "creating build/temp.linux-x86_64-3.6/src\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/content/warp-ctc/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c src/binding.cpp -o build/temp.linux-x86_64-3.6/src/binding.o -std=c++11 -fPIC -DWARPCTC_ENABLE_GPU -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_warp_ctc -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/src/binding.o -L/content/warp-ctc/build -L/usr/local/cuda/lib64 -lwarpctc -lcudart -o build/lib.linux-x86_64-3.6/warpctc_pytorch/_warp_ctc.cpython-36m-x86_64-linux-gnu.so -Wl,-rpath,/content/warp-ctc/build\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/warpctc_pytorch\n",
            "copying build/lib.linux-x86_64-3.6/warpctc_pytorch/__init__.py -> build/bdist.linux-x86_64/egg/warpctc_pytorch\n",
            "copying build/lib.linux-x86_64-3.6/warpctc_pytorch/_warp_ctc.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg/warpctc_pytorch\n",
            "byte-compiling build/bdist.linux-x86_64/egg/warpctc_pytorch/__init__.py to __init__.cpython-36.pyc\n",
            "creating stub loader for warpctc_pytorch/_warp_ctc.cpython-36m-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/warpctc_pytorch/_warp_ctc.py to _warp_ctc.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying warpctc_pytorch.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying warpctc_pytorch.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying warpctc_pytorch.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying warpctc_pytorch.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "warpctc_pytorch.__pycache__._warp_ctc.cpython-36: module references __file__\n",
            "creating dist\n",
            "creating 'dist/warpctc_pytorch-0.1-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing warpctc_pytorch-0.1-py3.6-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.6/dist-packages/warpctc_pytorch-0.1-py3.6-linux-x86_64.egg\n",
            "Extracting warpctc_pytorch-0.1-py3.6-linux-x86_64.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding warpctc-pytorch 0.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/warpctc_pytorch-0.1-py3.6-linux-x86_64.egg\n",
            "Processing dependencies for warpctc-pytorch==0.1\n",
            "Finished processing dependencies for warpctc-pytorch==0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "avq8bzmjrvD0",
        "colab_type": "code",
        "outputId": "a8e3b631-287c-4aaa-c557-d8277610ba45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "cell_type": "code",
      "source": [
        "# example for ctc loss\n",
        "import torch\n",
        "from warpctc_pytorch import CTCLoss\n",
        "ctc_loss = CTCLoss()\n",
        "# expected shape of seqLength x batchSize x alphabet_size\n",
        "probs = torch.FloatTensor([[[0.1, 0.6, 0.1, 0.1, 0.1], [0.1, 0.1, 0.6, 0.1, 0.1]]]).transpose(0, 1).contiguous()\n",
        "labels = torch.IntTensor([1, 2])\n",
        "label_sizes = torch.IntTensor([2])\n",
        "probs_sizes = torch.IntTensor([2])\n",
        "probs.requires_grad_(True)  # tells autograd to compute gradients for probs\n",
        "cost = ctc_loss(probs, labels, probs_sizes, label_sizes)\n",
        "cost.backward()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-e1e5b80214bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwarpctc_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCTCLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mctc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCTCLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# expected shape of seqLength x batchSize x alphabet_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'warpctc_pytorch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "k8Jt5oP9txDa",
        "colab_type": "code",
        "outputId": "fb3d505d-734c-4c3d-848d-5da12b9e9b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SeanNaren/deepspeech.pytorch.git\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deepspeech.pytorch'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 1199 (delta 1), reused 1 (delta 0), pack-reused 1188\u001b[K\n",
            "Receiving objects: 100% (1199/1199), 373.37 KiB | 688.00 KiB/s, done.\n",
            "Resolving deltas: 100% (752/752), done.\n",
            "audio  build  deepspeech.pytorch  sample_data  torchaudio.egg-info  warp-ctc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RPZcf_-8t8Vr",
        "colab_type": "code",
        "outputId": "75a5a43e-fd1a-464a-dfd5-1c26e4cd266d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "cell_type": "code",
      "source": [
        "# !cd deepspeech.pytorch; python transcribe.py --model_path models/deepspeech.pth.tar --audio_path /path/to/audio.wav\n",
        "!cd deepspeech.pytorch; python transcribe.py --audio-path /../001.wav"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"transcribe.py\", line 67, in <module>\n",
            "    model = DeepSpeech.load_model(args.model_path)\n",
            "  File \"/content/deepspeech.pytorch/model.py\", line 239, in load_model\n",
            "    package = torch.load(path, map_location=lambda storage, loc: storage)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/serialization.py\", line 365, in load\n",
            "    f = open(f, 'rb')\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'models/deepspeech_final.pth'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T4ALFL74uoT9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 新段落"
      ]
    },
    {
      "metadata": {
        "id": "697n_jP7gco7",
        "colab_type": "code",
        "outputId": "472bd213-59ac-47a3-b5a3-797b0374631a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "yXZKyjXBO_qL",
        "colab_type": "code",
        "outputId": "b64704d1-13b8-4c9c-a07d-d6d6aec858bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "!nvcc -V"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Tue_Jun_12_23:07:04_CDT_2018\n",
            "Cuda compilation tools, release 9.2, V9.2.148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MLX0EIXsOJFa",
        "colab_type": "code",
        "outputId": "239898ce-6e4b-49ce-c074-5ca6935840de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import DataParallel\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.optim as optim\n",
        "\n",
        "print(torch.__version__)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RN3q7r5gPcrN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "class constants(object):\n",
        "    def __init__(self):\n",
        "        self.PAD = 0\n",
        "        self.UNK = 1\n",
        "        self.BOS = 2\n",
        "        self.EOS = 3\n",
        "\n",
        "        self.PAD_WORD = '<pad>'\n",
        "        self.UNK_WORD = '<unk>'\n",
        "        self.BOS_WORD = '<s>'\n",
        "        self.EOS_WORD = '</s>'\n",
        "Constants = constants()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eh9SsjSoQsnx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        # input_dim = vocab_size + 4\n",
        "        # self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim,\n",
        "                          num_layers=num_layers, bidirectional=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_seqs, input_lengths, embedding, hidden=None):\n",
        "        # src = [sent len, batch size]\n",
        "        embedded = embedding(input_seqs)\n",
        "        # embedded = [sent len x batch size x emb dim]\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        outputs, hidden = self.rnn(packed, hidden)\n",
        "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        \n",
        "        del embedded, packed, output_lengths\n",
        "        \n",
        "        outputs = outputs[:,:,:self.hidden_dim] + outputs[:,:,self.hidden_dim:]\n",
        "        hidden = hidden[-1,:,:] + hidden[-2,:,:]\n",
        "        # outputs = [sent len, batch size, hid dim * n directions]\n",
        "        # hidden = [n layers, batch size, hid dim]\n",
        "        # outputs are always from the last layer\n",
        "        return outputs, hidden.unsqueeze(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1TTjWe3NRqsR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def attention(query, key, value=None):\n",
        "    \"\"\"query[B, H], key[B, L, H], value[B, L, H]\"\"\"\n",
        "    value = key\n",
        "    query = query.unsqueeze(1).repeat(1, key.size(1), 1)\n",
        "    # (B, L, H)\n",
        "    score = torch.sum(query * key, -1)\n",
        "    attn = score.softmax(-1).unsqueeze(1)\n",
        "    del score\n",
        "    # (B, 1, L)\n",
        "    outputs = torch.matmul(attn, value)\n",
        "    return outputs.squeeze(1), attn.squeeze(1)\n",
        "\n",
        "class DecoderWithAttn(nn.Module):\n",
        "    def __init__(self, voc_siz, emb_siz, hid_siz, num_layers=1, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_siz = emb_siz\n",
        "        self.hid_siz = hid_siz\n",
        "        self.voc_siz = voc_siz\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.rnn = nn.GRU(emb_siz, hid_siz,\n",
        "                          num_layers=num_layers)\n",
        "        self.out = nn.Linear(hid_siz * 2, voc_siz)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inp, h, embedding, enc_outputs):\n",
        "        # input = [bsz]\n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "        # encoder_outputs = [sent len, batch size, hid dim * n directions]\n",
        "        # input : [bsz] -> [1, bsz]\n",
        "        embedded = embedding(inp.unsqueeze(0))\n",
        "        # embeded = [1, bsz, emb dim]\n",
        "        _, hidden = self.rnn(embedded, h)\n",
        "        \n",
        "        # attn\n",
        "        # context: [bsz, hid], attn: [bsz, seq]\n",
        "        context, attn = attention(hidden[-1], enc_outputs.permute(1,0,2), value=None)\n",
        "        \n",
        "        emb_con = torch.cat([hidden[-1], context], dim=-1)\n",
        "        \n",
        "        output = self.out(self.dropout(emb_con)).log_softmax(-1)\n",
        "        # outputs = [sent len, batch size, vocab_size]\n",
        "        return output, hidden, attn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PFnCPHqmRwGJ",
        "colab_type": "code",
        "outputId": "f87b2754-f5f7-4e22-8b3e-63d358d1b0fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "cell_type": "code",
      "source": [
        "# example\n",
        "\n",
        "encoder = Encoder(input_dim=10, embedding_dim=20, hidden_dim=10)\n",
        "decoder = DecoderWithAttn(voc_siz=10, emb_siz=20, hid_siz=10)\n",
        "\n",
        "embedding = nn.Embedding(10, 20)\n",
        "\n",
        "inp = (torch.rand(2, 3).relu() * 4).long()\n",
        "# inp = Variable(inp, requires_grad=False)\n",
        "print(inp)\n",
        "\n",
        "outputs, h = encoder(inp.permute(1, 0), np.array([3, 3]), embedding)\n",
        "\n",
        "decoder(inp[:, 0], h, embedding, outputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 0, 1],\n",
            "        [1, 1, 3]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-2.4960, -2.0804, -2.3292, -2.1563, -2.8775, -2.1951, -2.0818, -2.6549,\n",
              "          -2.1324, -2.3188],\n",
              "         [-2.6771, -1.9753, -2.2301, -2.1704, -2.8830, -2.1328, -2.2388, -2.5547,\n",
              "          -2.2098, -2.2792]], grad_fn=<LogSoftmaxBackward>),\n",
              " tensor([[[ 0.2681, -0.4831,  0.2203,  0.1389, -0.1867, -0.1735,  0.2489,\n",
              "            0.0462, -0.4146, -0.0252],\n",
              "          [ 0.1834, -0.5881,  0.2708, -0.0189, -0.1866, -0.1978,  0.2123,\n",
              "           -0.0466, -0.3120, -0.0733]]], grad_fn=<ViewBackward>),\n",
              " tensor([[0.3027, 0.3448, 0.3525],\n",
              "         [0.3294, 0.3573, 0.3133]], grad_fn=<SqueezeBackward1>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "q84TTYObR7h5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, word_emb, encoder, decoder, teacher_forcing_ratio=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        global Constants\n",
        "        \n",
        "        self.word_emb = word_emb\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "        #self.cos = nn.CosineSimilarity(dim=2, eps=1e-6)\n",
        "        self.cos = nn.CosineEmbeddingLoss()\n",
        "        self.nll = nn.NLLLoss(ignore_index=Constants.PAD)\n",
        "        \n",
        "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
        "        self.lambda_ = 0.1\n",
        "        \n",
        "        \n",
        "    def forward(self, inp, src_lengths, mode=True):\n",
        "        global config, Constants, device\n",
        "        \n",
        "        enc_outputs, h = self.encoder(inp.permute(1, 0), src_lengths, self.word_emb)\n",
        "        \n",
        "        if mode:\n",
        "            # auto encode\n",
        "            loss, output = self.nllLoss(h, enc_outputs, target=inp)\n",
        "        else:\n",
        "            # abstract\n",
        "            loss, output = self.cosLoss(h, enc_outputs, target=inp)\n",
        "        \n",
        "        return loss, output\n",
        "        \n",
        "    def decode(self, h, enc_outputs, target, teacher_rate=1, use_mask=False):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "         - h:       1 x batch_siz x hid_siz, tensor\n",
        "         - enc_outputs: seq_len x batch_siz x hid_siz\n",
        "         - target:  batch_siz x seq_len, tensor\n",
        "        Output:\n",
        "         - outputs: seq_len x batch_siz x voc_siz, tensor\n",
        "        \"\"\"\n",
        "        global config, Constants, device\n",
        "        \n",
        "        batch_size = h.size(1)\n",
        "        num_word = config['siz_voc']\n",
        "        max_len = target.size(1)\n",
        "        \n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(max_len, batch_size, num_word).to(device)\n",
        "        outputs[1:, :, Constants.PAD] = 1\n",
        "        outputs[0, :, Constants.BOS] = 1                                    # seq_len x batch x vocab\n",
        "        outputs = outputs.log_softmax(-1)\n",
        "        \n",
        "        # end token\n",
        "        end_token = torch.zeros(batch_size)\n",
        "        # Coverage mechanism\n",
        "        c_t = torch.zeros(batch_size, enc_outputs.size(0)).to(device)\n",
        "        loss_cov = 0\n",
        "        # decoder\n",
        "        tar = target.permute(1, 0)\n",
        "        out = torch.ones(batch_size).long().to(device) * Constants.BOS\n",
        "        for t in range(1, max_len):                                        # skip sos\n",
        "            out, h, attn = self.decoder(out, h, self.word_emb, enc_outputs)\n",
        "            \n",
        "            loss_cov = loss_cov + torch.min(c_t, attn).sum()\n",
        "            c_t = c_t + attn\n",
        "            \n",
        "            outputs[t] = out\n",
        "            \n",
        "            # mask end token\n",
        "            if use_mask:\n",
        "                # record end token\n",
        "                out_copy = out.cpu().detach().max(-1)[1]\n",
        "                end_token += (out_copy == Constants.EOS).squeeze().type(end_token.type()) + \\\n",
        "                             (out_copy == Constants.PAD).squeeze().type(end_token.type())\n",
        "                if (end_token>0).sum() >= end_token.size(0):\n",
        "                    break\n",
        "                # mask\n",
        "                outputs[t, end_token>0, :] = 0\n",
        "                outputs[t, end_token>0, Constants.PAD] = 1\n",
        "                outputs[t, end_token>0, :] = outputs[t, end_token>0, :].log_softmax(-1)\n",
        "            \n",
        "            # next word\n",
        "            teacher_force = random.random() < teacher_rate\n",
        "            out = (tar[t] if teacher_force else out.max(-1)[1])\n",
        "        \n",
        "        del h, out, tar\n",
        "        \n",
        "        return outputs, loss_cov/t\n",
        "    \n",
        "    def nllLoss(self, h, enc_outputs, target):\n",
        "        \n",
        "        outputs, loss_cov = self.decode(h, enc_outputs, target, \n",
        "                                        teacher_rate=self.teacher_forcing_ratio)        # seq x batch(1) x voc\n",
        "        \n",
        "        loss = self.nll(\n",
        "            outputs[1:].contiguous().view(-1, outputs.size(-1)), \n",
        "            target.permute(1, 0)[1:].contiguous().view(-1))\n",
        "\n",
        "        return loss + self.lambda_ * loss_cov, outputs\n",
        "\n",
        "    def cosLoss(self, h, enc_outputs, target):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "         - h: 1 x batch x hid\n",
        "        \"\"\"\n",
        "        batch_siz = h.size(1)\n",
        "        max_len = target.size(1)\n",
        "        \n",
        "        enc_mean = enc_outputs.mean(1).unsqueeze(1)                                     # seq x 1 x hid\n",
        "        h_mean = h.mean(1).unsqueeze(1)                                                 # 1 x 1 x hid\n",
        "        \n",
        "        outputs, loss_cov = self.decode(h_mean, enc_mean, target[:1,:], teacher_rate=0)           # seq x batch(1) x voc\n",
        "        _, h_hat = self.encoder(\n",
        "            outputs.argmax(-1), \n",
        "            np.array([max_len] * outputs.size(1)), \n",
        "            self.word_emb) # 1 x 1 x hid\n",
        "        \n",
        "        h_hat = h_hat.squeeze(0)\n",
        "        y = torch.ones(batch_siz).to(device)\n",
        "        sim = self.cos(h_hat, h.squeeze(0), y)\n",
        "        \n",
        "        del h_hat, h_mean\n",
        "        return sim, outputs\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1lywOPtnSBE0",
        "colab_type": "code",
        "outputId": "dbe64584-eb28-465f-98c5-813d9f55facd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "# example\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "config = {\n",
        "    'use_cuda': True,\n",
        "    \n",
        "    'siz_voc': 10,  # word number\n",
        "    'siz_emb': 20,                     # embedding dim of word & tag\n",
        "    'siz_hid': 10,                     # hidden size of GRU\n",
        "    \n",
        "    'seq_len': 5,                      # max sequence length\n",
        "    'siz_batch': 64,                  # batch size of Seq2seq\n",
        "    'step_seq': 10000,\n",
        "    \n",
        "}\n",
        "\n",
        "\n",
        "encoder = Encoder(config[\"siz_voc\"], config[\"siz_emb\"], config[\"siz_hid\"]).to(device)\n",
        "decoder = DecoderWithAttn(config[\"siz_voc\"], config[\"siz_emb\"], config[\"siz_hid\"]).to(device)\n",
        "embedding = nn.Embedding(config[\"siz_voc\"], config[\"siz_emb\"]).to(device)\n",
        "\n",
        "model = Seq2Seq(embedding, encoder, decoder).to(device)\n",
        "\n",
        "\n",
        "inp = torch.randint(10, (2, 5)).long()\n",
        "# inp = Variable(inp, requires_grad=False)\n",
        "print(inp)\n",
        "\n",
        "nllloss, out_AE = model(inp, np.array([5, 4]), mode=True)\n",
        "cosloss, out_Avg = model(inp, np.array([5, 4]), mode=False)\n",
        "\n",
        "nllloss.backward()\n",
        "\n",
        "print(\"NLLLoss: \", nllloss)\n",
        "print(\"cosLoss: \", cosloss)\n",
        "\n",
        "print(\"AE: \", out_AE.size())\n",
        "print(\"abstract: \", out_Avg.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 4, 6, 0, 7],\n",
            "        [5, 5, 4, 7, 2]])\n",
            "NLLLoss:  tensor(2.6199, grad_fn=<ThAddBackward>)\n",
            "cosLoss:  tensor(0.1302, grad_fn=<DivBackward0>)\n",
            "AE:  torch.Size([5, 2, 10])\n",
            "abstract:  torch.Size([5, 1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nH19chFUSYGL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# data\n"
      ]
    },
    {
      "metadata": {
        "id": "LJFr-2laSV3Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from numpy import linalg as LA\n",
        "import pickle as pickle\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, vocab_file, emb_file='', dim_emb=0):\n",
        "        with open(vocab_file, 'rb') as f:\n",
        "            self.size, self.word2id, self.id2word = pickle.load(f)\n",
        "        self.dim_emb = dim_emb\n",
        "        self.embedding = np.random.random_sample(\n",
        "            (self.size, self.dim_emb)) - 0.5\n",
        "\n",
        "        if emb_file:\n",
        "            print('Loading word vectors from', emb_file)\n",
        "            with open(emb_file) as f:\n",
        "                for line in f:\n",
        "                    parts = line.split()\n",
        "                    word = parts[0]\n",
        "                    vec = np.array([float(x) for x in parts[1:]])\n",
        "                    if word in self.word2id:\n",
        "                        self.embedding[self.word2id[word]] = vec\n",
        "\n",
        "        for i in range(self.size):\n",
        "            self.embedding[i] /= LA.norm(self.embedding[i])\n",
        "\n",
        "def build_vocab(data, path, min_occur=5):\n",
        "    word2id = {'<pad>':0, '<go>':1, '<eos>':2, '<unk>':3}\n",
        "    id2word = ['<pad>', '<go>', '<eos>', '<unk>']\n",
        "\n",
        "    words = [word for sent in data for word in sent]\n",
        "    cnt = Counter(words)\n",
        "    for word in cnt:\n",
        "        if cnt[word] >= min_occur:\n",
        "            word2id[word] = len(word2id)\n",
        "            id2word.append(word)\n",
        "    vocab_size = len(word2id)\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump((vocab_size, word2id, id2word), f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "57iiYvGEYfKR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_max = 20000\n",
        "train0 = load_sent(args.train + '.0', num_max)\n",
        "train1 = load_sent(args.train + '.1', num_max)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}